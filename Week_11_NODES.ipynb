{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of Neural ODEs or How I Learned the Velocity of a Falling Object\n",
    "\n",
    "If you recall from your introductory physics class, the velocity of a falling ball at some time with quadratic drag is given by, \n",
    "\n",
    "$$ \\frac{dv}{dt} = \\theta_1 - \\theta_2 v^{2} $$\n",
    "\n",
    "This is simply Newton's law stating that the change in velocity with respect to time is given by the gravitational constant $\\theta_1$ (which works to speed up the ball) minus some drag coefficient $\\theta_2$ times the square of the velocity (which works to reduce the speed of the ball when it's going fast). \n",
    "\n",
    "First, let's implement the forward model.  Below, you will need to code just the right hand side of the equation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import odeint #This will solve our ODEs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "def dvdt(v, t, thetas): # velocity of the falling baseball from newton\n",
    "    ### TODO implement the dynamics function provided above given a list of thetas\n",
    "    ### Hint: The t argument is just present to make the ode solver happy. It is not actually used in the function.\n",
    "    return ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a function that describes how $v$ changes through time, we can use the scipy package odeint to perform numerical integration through time and produce an approximate numerical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = np.linspace(0, 3, 50) # These are the times at which we would like to output our solution \n",
    "true_thetas = [10, 0.2] # These are the parameters that we will take to be \"true\".  Later, we will try to recover them\n",
    "initial_velocity = 0 #The initial condition of our velocity\n",
    "\n",
    "out = odeint(dvdt, initial_velocity, times, (true_thetas,)) #Integrate our dynamics function outputting at each time\n",
    "v_true = out[:,0]\n",
    "\n",
    "plt.figure(figsize =(10,7))\n",
    "plt.plot(times, v_true, '-', label = 'Data') #plot the data and the inital guess\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Velocity (m/s)')\n",
    "plt.title('True Ball Velocity')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's imagine that we didn't know the true parameters, but wanted to recover them from data.  We can simulate some noisy data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some noise to give us our true 'observations'\n",
    "v_train = v_true + np.random.randn(v_true.size)*0.5\n",
    "\n",
    "plt.figure(figsize =(10,7))\n",
    "plt.plot(times, v_train, '.', label = 'Data') #plot the data and the inital guess\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Velocity (m/s)')\n",
    "plt.title('Ball Velocity Data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a wrong guess as to what our parameters might be, and see how well we predict the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's pretend like we don't know the true values of our thetas.\n",
    "guess_thetas = [???, ???] #Make an inital guess as to what the parameters might be.\n",
    "\n",
    "### This is for plotting later\n",
    "meshv = np.linspace(0, 9, 27)\n",
    "mesht = np.linspace(0, 3, 12)\n",
    "meshv, mesht = np.meshgrid(meshv,mesht)\n",
    "changet = np.ones(meshv.shape)*5\n",
    "\n",
    "out = odeint(dvdt, initial_velocity, times, (guess_thetas,))\n",
    "v_guess = out[:,0]\n",
    "\n",
    "plt.figure(figsize =(10,7))\n",
    "plt.plot(times, v_train, '.', label = 'Data') #plot the data and the inital guess\n",
    "plt.plot(times, v_guess, '--', label = 'Guess')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Velocity (m/s)')\n",
    "plt.title('Ball Velocity Data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe your guess is good, but maybe it's not.  As always in ML, we need to train our parameters by minimizing a loss function.  For our purposes, the loss will be sum squared error:\n",
    "$$ L = \\sum_{t_i \\in t_{obs}} \\frac{1}{2} \\left( v(t_i) - v_{obs,i}\\right)^2.$$\n",
    "\n",
    "We would like to use the adjoint method to compute the gradient of this cost function with respect to the parameters $\\theta_0$ and $\\theta_1$.  To begin, we will need to derive the continuous analog of the chain rule, namely the adjoint, which gives us $a(t) = \\frac{\\partial L}{\\partial v}$ at any point in time.  Equation 4from Chen provides us with the adjoint equation, which is another ODE that we can solve:\n",
    "$$ \\frac{\\partial a}{\\partial t} = -a(t) \\frac{\\partial f(v,t,\\theta)}{\\partial v}, $$ where $f(v,t,\\theta)$ is the right hand side of the ODE.\n",
    "\n",
    "### Question 1: Show that\n",
    "$$ \\frac{\\partial f(v,t,\\theta)}{\\partial v(t)} = -2 \\theta_1 v(t) $$\n",
    "\n",
    "Note that this equation depends on $v(t)$!  Thus we either have to store $v(t)$ from the forward pass (which is equivalent to reverse mode AD), or we have to solve the equation for $v(t)$ backwards in time along with the adjoint equation, which is what we do here.\n",
    "\n",
    "If we had a continuous cost function, the adjoint equation would also have a source term.  However, because our observations are discrete, we have a discontinous jump in the value of the adjoint at every observation.  The natural way of handling this discontinuity is to solve the adjoint equation backwards in time over every interval between observations, with initial condition $a(t_i)^+$ given by\n",
    "$$\n",
    "a(t_i)^+ = a(t_i)^- + v(t_i) - v_{obs}(t_i),\n",
    "$$\n",
    "\n",
    "where $a(t_i)^{-}$ is the adjoint at $t_i$ before we see the observation.  In words, this says that we should solve the adjoint equation backwards in time until we encounter an observation.  When we encounter an observation, we add the residual between the modeled and observed velocity at that point to the adjoint (discontinuously) and then continue solving backwards in time until we get to the next observation, and so on.  This discontinuous treatment can be seen in the adjoint method below.  \n",
    "\n",
    "With the adjoint in hand, we can compute the gradients of the cost with respect to the parameters $\\theta_1$ and $\\theta_2$ using Equation 5 from Chen.  If take the derivative of Eq. 5 with respect to time, we get another ODE for each parameter's gradient:\n",
    "$$\n",
    "\\frac{\\partial g_{\\theta_k}}{\\partial t} = -a(t) \\frac{\\partial f(v,t,\\theta)}{\\partial \\theta_k},\n",
    "$$\n",
    "where $g_{\\theta_k} = \\frac{\\partial L}{\\partial \\theta_k}$.  These ODEs are easy to solve by appending them to the adjoint.  Thus, for this problem the backward pass involves solving a system of four ODEs simultaneously: the adjoint equation, the forward model (but backwards in time), and an ODE for each parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FallingVelocity():\n",
    "    \n",
    "    def __init__(self, observations, thetas, times):\n",
    "        \n",
    "        self.observations = observations #our observation velocity\n",
    "        self.thetas = thetas  #list of initial params must have opposite signs!\n",
    "        self.times = times #our observation times\n",
    "        \n",
    "        \n",
    "    def _dvdt(self, v, t): ### velocity of the falling baseball from newton : copied from above\n",
    "        return self.thetas[0] - self.thetas[1]*v**2\n",
    "    \n",
    "    def _dadt(self, augmented_state, t):\n",
    "        v = augmented_state[0]\n",
    "        a = augmented_state[1]\n",
    "        dLdtheta0 = augmented_state[2] #You won't use this\n",
    "        dLdtheta1 = augmented_state[3] #You won't use this either\n",
    "        \n",
    "        return np.array([self.thetas[0] - self.thetas[1]*v**2,     # RHS forward model\n",
    "                         ???,                                      # RHS adjoint equation\n",
    "                         -a,                                       # integrand for theta_0\n",
    "                         ???])                                     # integrand for theta_1\n",
    "\n",
    "    def forward(self):\n",
    "        #Forward pass integration to get final velocity\n",
    "        out = odeint(self._dvdt, 0, self.times)\n",
    "        v_pred = out[:,0]       \n",
    "        return v_pred    #all velocities are stored merely for ease of plotting purposes.\n",
    "\n",
    "    def adjoint(self, output):\n",
    "        currentvel = output #initial velocity \n",
    "        \n",
    "        time_backwards = self.times[::-1] #since we are integrating backwards in time   \n",
    "        observations_backward = self.observations[::-1]\n",
    "        augmented_state = [np.array([currentvel,observations_backward[0]-currentvel,0,0])]\n",
    "        \n",
    "        for i in range(len(self.times)-1): #Solve odes between each datapoint\n",
    "                       \n",
    "            ## Here is where we integrate backwards through the network\n",
    "            \n",
    "            backward_time_range = time_backwards[i:(i+2)]\n",
    "            \n",
    "            ###TODO add ajoint for theta1 \n",
    "            backward = odeint(self._dadt,augmented_state[-1],backward_time_range)[-1].ravel()\n",
    "            augmented_state.append(np.array([backward[0],\n",
    "                                             backward[1] + backward[0] - observations_backward[i+1],\n",
    "                                             backward[2],\n",
    "                                             backward[3]]))     \n",
    "            \n",
    "        return augmented_state[-1][2], augmented_state[-1][3] #??? What are we returning here?\n",
    "    \n",
    "model = FallingVelocity(v_train, [4, 1.], times)\n",
    "\n",
    "eta = 0.0001 # Step size to take/learning rate. Breaks pretty easily, be careful here\n",
    "\n",
    "for i in range(6500):\n",
    "\n",
    "    output = model.forward()\n",
    "    \n",
    "    grad_theta0, grad_theta1 = model.adjoint(output[-1]) ### Only feed the model the final state for backward pass\n",
    "    \n",
    "    ### TODO update thetas\n",
    "    model.thetas[0] -= ???\n",
    "    model.thetas[1] -= ???\n",
    "       \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize = (15, 7))\n",
    "        plt.subplot(121)\n",
    "        plt.plot(times, v_train, '.r', label='Observations')\n",
    "        plt.plot(times, output, 'b', label='Leared Dynamics', linewidth = 2)\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Velocity (m/s)')\n",
    "        plt.legend()\n",
    "        plt.subplot(122)\n",
    "        changev = dvdt(meshv, mesht, model.thetas)\n",
    "        plt.quiver(mesht, meshv, changet, changev, width = .005)\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Velocity (m/s)')\n",
    "        plt.title('Underlying Dynamics')\n",
    "        plt.show()\n",
    "        print('Iter {:04d}'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: What would happen the velocity trajectory if we only had observations of the initial and final state?\n",
    "\n",
    "### Question 3: Run the model again, but alter the observation data times so they are no longer evenly spaced. Does this method still work?\n",
    "\n",
    "## Let's find the velocity again with a neural network\n",
    "For this section, we'll use the  `torchdiffeq` package, which was created by the authors of the paper.  You can install it with `pip install torchdiffeq`\n",
    "\n",
    "Now, we can also ask the question: \"what if we didn't know the right hand side of the ODE a priori?\" what should we do then?  In the previous example, we explicitly knew the ODE that described the dynamics of the velocity.\n",
    "\n",
    "$$ \\frac{dv}{dt} = \\theta_1 + \\theta_2 v^{2} $$\n",
    "\n",
    "If we don't know the ODE that describes the velocity, we can represent it with a neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchdiffeq import odeint as odeint_nn\n",
    "from torchdiffeq import odeint_adjoint ## Can use this if we want to use the adjoint method\n",
    "\n",
    "# NN that will be used to approximate dvdt \n",
    "class ODEFunc(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ODEFunc, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 1),\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        return self.net(y)\n",
    "\n",
    "    \n",
    "# Number of time steps to integrate forward\n",
    "batch_time = 10\n",
    "# Number of batch samples to use in each gradient descent step\n",
    "batch_size = 10\n",
    "\n",
    "# This is how minibatch is imiplemented in the torchdiffeq examples. Interestingly, the model can get good \n",
    "# results by only training on 10 data points at a time.\n",
    "def get_batch(y, t):\n",
    "    \n",
    "    true_y = torch.Tensor(y)\n",
    "    t = torch.Tensor(t)\n",
    "    \n",
    "    s = torch.from_numpy(np.random.choice(np.arange(len(t) - batch_time, dtype=np.int64), batch_size, replace=False))\n",
    "    batch_y0 = true_y[s]  # (M, D)\n",
    "    batch_t = t[:batch_time]  # (T)\n",
    "    batch_y = torch.stack([true_y[s + i] for i in range(batch_time)], dim=0)  # (T, M, D)\n",
    "    return batch_y0, batch_t, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vel_train2 = vel_train[:,np.newaxis]\n",
    "\n",
    "model = ODEFunc()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "\n",
    "for itr in range(1000):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get initial state, time steps, true y values at each timestep.\n",
    "    batch_y0, batch_t, batch_y = get_batch(vel_train2, times)\n",
    "\n",
    "    # TODO: Use NODE to integrate forward\n",
    "    # Hint: The odeint implemented in the torchdiffeq package uses the same syntax as the scipy odeint function.\n",
    "    #       Only now, we are using a NN instead of a predefined dvdt function.\n",
    "    pred_y = odeint_nn(???)\n",
    "    # NOTE: We are just backpropagating throught the ODE solver here. However, if we want to use the \n",
    "    #       adjoint method implemented in torchdiffeq, all we have to do is call odeint_adjoint() rather\n",
    "    #       than odeint_nn(). Both functions use the same syntax.\n",
    "    \n",
    "    \n",
    "    # loss = torch.sqrt(torch.mean((pred_y - batch_y) ** 2))\n",
    "    loss = torch.mean(torch.abs(pred_y - batch_y))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if itr % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        with torch.no_grad():\n",
    "            plt.figure(figsize = (15, 7))\n",
    "            plt.subplot(121)\n",
    "            # TODO: Run model forward from time zero at all time steps so we can track progress\n",
    "            pred_y = odeint_nn(???)\n",
    "            loss = torch.mean(torch.abs(pred_y - torch.Tensor(vel_train2)))\n",
    "            plt.plot(times, vel_train2, '.r', label='Observations')\n",
    "            plt.plot(times, pred_y, 'b', label='NODE Model')\n",
    "            plt.xlabel('Time (s)')\n",
    "            plt.ylabel('Velocity (m/s)')\n",
    "            plt.legend()\n",
    "            plt.subplot(122)\n",
    "            \n",
    "            for i in range(4*3):\n",
    "                for j in range(9*3):\n",
    "                    changev = model.forward(i/3, torch.tensor([float(j/3)]))\n",
    "                    plt.quiver(i/3, j/3, 5, changev, width = .005, headwidth = 3)\n",
    "            plt.xlabel('Time (s)'),\n",
    "            plt.ylabel('Velocity (m/s)')\n",
    "            plt.title('Underlying Dynamics')\n",
    "            plt.show()\n",
    "            print('Iter {:04d} | Loss {:.6f}'.format(itr, loss.item()))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 4: Try swapping odeint_nn with odeint_adjoint and retraining the model. Is there a change in runtime or number of iterations before convergence? Why do you think we are seeing these differences between the two methods?\n",
    "\n",
    "### Question 5: Why might this example be a bad problem to solve with the adjoint method?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
